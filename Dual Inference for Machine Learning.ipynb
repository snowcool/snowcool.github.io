{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dual Inference for Machine Learning\n",
    "IJCAI-17\n",
    "\n",
    "Yingce Xia, Jiang Bian, Tao Qin, Nenghai Yu and Tie-Yan Liu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. 现有AI任务 中存在很多 对偶任务；\n",
    "1. 在实际应用中，对偶性 可以 提供 重要知识 来提高学习任务的性能。\n",
    "1. 现有的研究集中在 利用 对偶性 促进 训练过程 获得更强大的模型。但是很少利用这种关系 做 inference 任务。\n",
    "    1. Neural Machine Translation (NMT)\n",
    "    1. Sentiment Analysis\n",
    "    1. Image Processing \n",
    "1. 如何构建对偶性？train 模型的时候，最小化 元问题 与 对偶问题 的 loss.  利用参数 $\\alpha$ and $\\beta$ to balance the tradeoff between two losses. \n",
    "1. **Why it works?**\n",
    "An intuitive explanation of the magic of dual inference lies in that it leverages the **reconstruction ability**, which indicates that a matched pair $(x; y)$ should **not only get smaller loss for primal task, but also for dual task**.\n",
    "1. **We would like to point out that such improvement are achieved without changing/re-training the original primal and dual models.**\n",
    "本质是 在做一个方向的 inference 任务时，综合考虑到 逆向结果的信息。\n",
    "1. **The performance of dual inference does not highly depend on the model structures of the two dual tasks.** <br>\n",
    "性能不依赖于模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duality does exist between many pairs of AI tasks in realworld. For example, \n",
    "* machine translation [Wu et al., 2016] from one language to another, English-to-French, and that of the opposite direction, e.g. French-to-English, form up a typical dual form; \n",
    "* speech recognition [Graves et al., 2013; Amodei et al., 2016] and speech generation/synthesis [Oord et al., 2016] constitute a duality in the domain of speech processing; \n",
    "* besides, a specific pair of dual tasks in face attribute manipulation can be comprised of the task of removing glass from face and that of wearing glass to face [Shen and Liu, 2016]. \n",
    "\n",
    "* Beyond, advanced deep learning algorithms can formulate those pairs of tasks without explicit duality, such as image classification [He et al., 2016b; 2016c] and conditional image generation [van den Oord et al., 2016b; 2016a], into the dual form.\n",
    "\n",
    "**As common in real-world applications, the duality can provide vital knowledge for enhancing learning tasks.**\n",
    "\n",
    "* A recent study [He et al., 2016a] has magnified its importance by introducing a new dual learning framework to **boost** the learning of two tasks in the dual form. \n",
    "    * In particular, by leveraging unlabeled data, this work exploited the structure duality in machine translation to design a two-player game with a closedloop feedback system as a dual Neural Machine Translation (dual-NMT) algorithm. \n",
    "* [Xia et al., 2017] explored duality for supervised learning, whose idea is to increase the probabilistic consistency of the two dual tasks. \n",
    "* Another effort [Shen and Liu, 2016] attempted to combine duality with adversarial training to improve the performance of face attribute manipulation.\n",
    "\n",
    "\n",
    "To the best of our knowledge, all existing studies regarding the duality focus on utilizing it to boost the training process\n",
    "so as to obtain more powerful models. \n",
    "\n",
    "However, there has been little investigation on how to leverage this invaluable relationship into the **inference** stage of AI tasks. Intuitively, we have high confidence to judge $y$ is a good output for the input $x$ in the primal task, if $x$ is a good output for $y$ in the dual task. \n",
    "\n",
    "Therefore, in this paper, we propose a general framework of dual inference which can take advantage of both existing models from two dual tasks, without re-training, to conduct inference for each individual task.\n",
    "\n",
    "To better illustrate the high effectiveness of dual inference, we apply it into dual AI tasks in three particular domains:\n",
    "1. Neural Machine Translation (NMT): Translation from a source language into a target language naturally yields a dual task of inverse translation from the target to the source. NMT, emerging as widely-used approach, employs a Recurrent Neural Network based encoder-decoder framework to model the probability of a sentence in target language conditioned on the sentence in source one.\n",
    "1. Sentiment Analysis: Sentiment classification, aiming at predict the sentiment label of sentences, is a popular primal task in the domain of sentiment analysis. The corresponding dual task, is indeed sentence generation, whose objective is to automatically generate sentences based on a pre-designed sentiment.\n",
    "1. Image Processing: Image classification, the goal of which is to predict the label of an image, is one of major AI tasks in the domain of image processing. The dual task of image classification is obviously image generation, which is an emerging AI task to automatically generate images based on category labels.\n",
    "\n",
    "Empirical studies on dual tasks under these three specific domains have shown that dual inference can significantly improve the inference performance of each of individual tasks. **We would like to point out that such improvement are achieved without changing/re-training the original primal and dual models.** Moreover, we provide theoretical discussions to provide better understanding on dual inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to build duality?\n",
    "> 如何构建对偶性？train 模型的时候，最小化 元问题 与 对偶问题 的 loss.  利用参数 $\\alpha$ and $\\beta$ to balance the tradeoff between two losses. \n",
    "\n",
    "There are various potential rules to formulate duality into the dual inference framework. \n",
    "\n",
    "In this paper, we employ a most natural and straightforward approach, which \n",
    "* first combines the loss functions of the primal task and the dual task \n",
    "* and then selects the output that can **minimize the combined loss** as the inference result. \n",
    "\n",
    "More formally, we have following dual inference equations for the primal and dual task, respectively:\n",
    "<img style=\"float: center;\" src=\"pics/Dual Inference for Machine Learning-1.PNG\" width=\"40%\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\alpha$ and $\\beta$ are hyperparameters to balance the tradeoff between two losses, and they will be tuned based on performance on a validation set. \n",
    "\n",
    "Note that we do **not re-train or make any change** on the models of both primal and dual tasks.\n",
    "Most of inference rules, currently widely-used in machine learning tasks, can be described as below.\n",
    "\n",
    "$$f(x)=\\arg\\min_{y'\\in \\mathcal{Y}} l_f(x,y'); g(y)=\\arg\\min_{x'\\in \\mathcal{X}} l_g(x',y)$$\n",
    "\n",
    "which are extreme cases in dual inference by setting $\\alpha$ and $\\beta$ to one. From this perspective, dual inference can be viewed as a more general inference framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that, a branch of inference rules using multiple models correspond to the ensemble [Opitz and Maclin, 1999] methods. However, dual inference yields a crucial difference from the ensemble. <!???>\n",
    "\n",
    "In particular, all models in an ensemble framework follow the same mapping direction, thus they can only serve for either the primal or the dual task, whilst the two models applied in the dual inference framework serve for both the primal and the dual tasks with opposite mapping directions, simultaneously.\n",
    "\n",
    "To gain better understanding of dual inference, in the following, we apply this new framework into dual AI tasks in three particular domains and conduct corresponding empirical studies to examine the effectiveness of dual inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Machine Translation\n",
    "\n",
    "As a state-of-the-art approach, neural machine translation (NMT) is a deep learning based end to end approach for machine translation. \n",
    "\n",
    "NMT models the conditional probability $P(y|x;\\theta)$ of a sentence $y$ in target language given a sentence $x$  in source language, and the parameter $θ$ is learned based on the training data consisting of a set of **bilingual** sentence pairs.\n",
    "\n",
    "During the typical inference step, given a source sentence $x$, NMT finds the target sentence $y$ with **largest conditional probability $P(y|x;\\theta)$** as the translation of $x$.\n",
    "\n",
    "Since the number of candidate target sentences is **exponentially** large, it usually employs **beam search** to find a reasonable target $y$ more efficiently. <br><!???>\n",
    "\n",
    "Due to the natural existence in NMT, structural duality has been exploited into the learning process of NMT [He et al., 2016a]. However, there has been little investigation on **how to leverage duality into the inference stage**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following the widely used work [Bahdanau et al., 2015], the loss functions used for inference in two directions, represented as $l_f$ and $l_g$ respectively, are specialized as negative log-likelihood in machine translation. Mathematically,\n",
    "\n",
    "$$l_f(x,y)=-\\log P(y|x;f)$$ $$l_g(x,y)=-\\log P(x|y;g)$$\n",
    "\n",
    "The dual inference for the primal task of neural machine translation is shown as follows:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;\" src=\"pics/Dual Inference for Machine Learning-2.PNG\" width=\"40%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the primal task and the dual one are of equal position in machine translation, the dual inference algorithm for the dual task can be defined in the same way, and we only show the algorithm for the primal task due to the limited space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Experimental Setup\n",
    "We employ two training methods, as described below, to obtain the NMT models: \n",
    "* **RNNSearch** represents the standard sequence-to-sequence training method as introduced in [Bahdanau et al., 2015; Jean et al., 2015].\n",
    "* **dual-NMT** denotes the dual learning as that proposed in [He et al., 2016a].\n",
    "\n",
    "The translation qualities are evaluated by tokenized case-sensitive BLEU [Papineni et al., 2002] scores calculated by multi-bleu.pl. The larger BLEU is, the better the translation quality is.\n",
    "\n",
    "**Conclusion:**\n",
    "1. From this figure, we can see that **dual inference** can outperform the **standard inference rule**, i.e., $α = 1$ or $β = 1$, in a wide value range of $α$ and $β$.\n",
    "2. Case study: \n",
    "In the following, we leverage a specific example in $De \\rightarrow En$ translation to illustrate how dual inference can improve the performance of NMT. \n",
    "    * Let $y$ denote the German sentence, $x_1$ and $x_2$ denote $2$ out of $12$ candidates generated by beam search, \n",
    "    * $x^∗$ represent the ground-truth translation. \n",
    "    * Then, let $f$ denote the $En \\rightarrow De$ model and $g$ denote the $De \\rightarrow En$ model.\n",
    "\n",
    "    The examples and the loss of corresponding sentence pairs are shown in Table 2.\n",
    "    <img style=\"float: center;\" src=\"pics/Dual Inference for Machine Learning-3.PNG\" width=\"40%\">\n",
    "    From these results, we can find that, **standard inference** ($\\alpha=1$, $\\beta=1$) rule prefers $x_1$ which is, however, quite a bad translation; in contrast, dual inference, by considering the probabilities of both directions, tends to boost a better translation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Discussions about NMT with reconstruction\n",
    "An intuitive explanation of the magic of dual inference lies in that it leverages the **reconstruction ability**, which indicates that a matched pair $(x; y)$ should **not only get smaller loss for primal task, but also for dual task**. \n",
    "\n",
    "The similar idea is also used for NMT by [Tu et al., 2017]. \n",
    "> * In that work, for any sentence pair $(x; y)$, the training objective is re-designed to minimize $l_{rec} = \\log \\mathbb{P}(y|x; \\theta) + \\log \\mathbb{P}(x|s; \\gamma)$, where $\\theta$ and $\\gamma$ are the parameters to learn, and $s$ is a hidden representation related to $x$ and $y$. \n",
    "* The inference phase in that work consists of first using model $\\theta$ to generate $K$ candidates and then selecting the sentence that can minimize $l_{rec}$. The reconstructor $\\gamma$ is only served for $\\theta$ without capability to make translations by itself. \n",
    "\n",
    "On the contrary, in the dual inference framework, the translation models of two directions can both make translations and can be trained individually. Furthermore, dual inference can improve the performance for both $f$ and $g$, which makes it quite different from the work in [Tu et al., 2017]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Sentiment Analysis\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
